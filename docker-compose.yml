services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on: [zookeeper]
    ports:
      - "29092:29092"   # external (host)
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql:ro

  spark-master:
    image: ${SPARK_IMAGE:-apache/spark:3.5.1-python3}
    container_name: spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "--host", "spark-master"]
    environment:
      SPARK_NO_DAEMONIZE: "true"
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./services/spark/app:/opt/spark-apps
      - ./data:/data

  spark-worker:
    image: ${SPARK_IMAGE:-apache/spark:3.5.1-python3}
    container_name: spark-worker
    depends_on: [spark-master]
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    environment:
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_CORES: 2
      SPARK_NO_DAEMONIZE: "true"
    ports:
      - "8081:8081"
    volumes:
      - ./data:/data

  spark-streaming:
    build:
      context: ./services/spark
      dockerfile: streaming.Dockerfile
    depends_on:
      kafka:
        condition: service_started
        required: true
      postgres:
        condition: service_started
        required: true
      spark-master:
        condition: service_started
        required: true
    environment:
      CHECKPOINT_AGG: /data/checkpoints/agg
      CHECKPOINT_RAW: /data/checkpoints/raw
      KAFKA_BROKER: kafka:9092
      KAFKA_TOPIC: rides
      KAFKA_STARTING_OFFSETS: earliest
      KAFKA_FAIL_ON_DATA_LOSS: "false"
      WINDOW_SIZE: "1 minute"
      WATERMARK: "30 seconds"
      RAW_PARQUET_PATH: /data/raw/rides
      POSTGRES_DB: rtp
      POSTGRES_PASSWORD: rtp_password
      POSTGRES_URL: jdbc:postgresql://postgres:5432/rtp
      POSTGRES_USER: rtp
      PYSPARK_DRIVER_PYTHON: python3
      PYSPARK_PYTHON: python3
      SLACK_WEBHOOK_URL: ${SLACK_WEBHOOK_URL:-}
      AGG_WRITE_STRATEGY: complete_overwrite
    volumes:
      - ./services/spark/app:/opt/spark-apps:ro
      - ./data:/data
    command:
      - /opt/spark/bin/spark-submit
      - --master
      - spark://spark-master:7077
      - --packages
      - org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.postgresql:postgresql:42.7.3
      - /opt/spark-apps/streaming_job.py
      
  producer:
    build: ./services/producer
    depends_on: [kafka]
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER}
      - KAFKA_TOPIC=${KAFKA_TOPIC}
    ports:
      - "8000:8000"

  streamlit:
    build: ./services/streamlit
    depends_on: [postgres]
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=${POSTGRES_PORT}
    ports:
      - "${STREAMLIT_PORT}:8501"

  airflow-init:
    build: ./services/airflow
    depends_on: [postgres]
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    entrypoint: /bin/bash -c
    command: >
      "airflow db migrate &&
       airflow users create
         --username admin
         --firstname Admin
         --lastname User
         --role Admin
         --email admin@example.com
         --password admin || true"
    volumes:
      - ./services/airflow/dags:/opt/airflow/dags
      - ./services/spark/app:/opt/airflow/spark-apps:ro
      - ./data:/data

  airflow:
    build: ./services/airflow
    depends_on:
      - spark-master
    environment:
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__CORE__EXECUTOR: "SequentialExecutor"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
    ports:
      - "8088:8080"
    volumes:
      - airflow_home:/opt/airflow
      - ./services/airflow/dags:/opt/airflow/dags
      - ./services/spark/app:/opt/airflow/spark-apps:ro
      - ./data:/data
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      bash -lc "airflow db migrate &&
      airflow webserver -p 8080 & airflow scheduler"

  prometheus:
    image: prom/prometheus:v2.54.0
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command: ["--config.file=/etc/prometheus/prometheus.yml"]
    depends_on: [producer, spark-streaming]
    ports:
      - "9090:9090"

  promtail:
    image: grafana/promtail:2.9.3
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - ./monitoring/promtail-config.yml:/etc/promtail/config.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock

  grafana:
    image: grafana/grafana:10.4.5
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
      - loki
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin

  loki:
    image: grafana/loki:2.9.3
    command: -config.file=/etc/loki/local-config.yaml
    ports:
      - "3100:3100"

volumes:
  pgdata:
  airflow_home: